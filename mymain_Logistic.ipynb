{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glen/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the file shows the fields separated by space and quoted by \", \n",
    "#and that where someone has included a quoted string in their review \n",
    "#there is a backslash before the quotes to escape them\n",
    "df = pd.read_csv('data.tsv', sep=' ', quotechar='\"', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>9245</td>\n",
       "      <td>8403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>35118</td>\n",
       "      <td>40376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>28666</td>\n",
       "      <td>19247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split_1  split_2  split_3\n",
       "0        2     9245     8403\n",
       "1        3    35118    40376\n",
       "2        4    28666    19247"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing set based on the splits,csv file\n",
    "# Each column consist of 25,000 test samples ID, so we can separate the data into three different train/test samples\n",
    "split = pd.read_csv('splits.csv', sep='\\t', quotechar='\"', escapechar='\\\\')\n",
    "split.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select s to determine which train and test split (S = 1 or 2 or 3)\n",
    "s = 1\n",
    "\n",
    "train_list = []\n",
    "test_list  = []\n",
    "# Use left join to seperate our data \n",
    "test1 = df.merge(split[['split_1']], left_on='new_id', right_on='split_1', how='inner')\n",
    "test1 = test1.drop('split_1',axis = 1)\n",
    "test1 = test1.reset_index(drop=True)\n",
    "\n",
    "#Select the rows not in test1 id\n",
    "train1 = df.loc[~df['new_id'].isin(test1['new_id'])]\n",
    "train1 = train1.reset_index(drop=True)\n",
    "\n",
    "# Use left join to seperate our data \n",
    "test2 = df.merge(split[['split_2']], left_on='new_id', right_on='split_2', how='inner')\n",
    "test2 = test2.drop('split_2',axis = 1)\n",
    "test2 = test2.reset_index(drop=True)\n",
    "\n",
    "#Select the rows not in test1 id\n",
    "train2 = df.loc[~df['new_id'].isin(test2['new_id'])]\n",
    "train2 = train2.reset_index(drop=True)\n",
    "\n",
    "# Use left join to seperate our data \n",
    "test3 = df.merge(split[['split_3']], left_on='new_id', right_on='split_3', how='inner')\n",
    "test3 = test3.drop('split_3',axis = 1)\n",
    "test3 = test3.reset_index(drop=True)\n",
    "\n",
    "#Select the rows not in test1 id\n",
    "train3 = df.loc[~df['new_id'].isin(test3['new_id'])]\n",
    "train3 = train3.reset_index(drop=True)\n",
    "\n",
    "train_list.append(train1)\n",
    "train_list.append(train2)\n",
    "train_list.append(train3)\n",
    "\n",
    "test_list.append(test1)\n",
    "test_list.append(test2)\n",
    "test_list.append(test3)\n",
    "\n",
    "train = train_list[s-1]\n",
    "test = test_list[s-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_review = review_to_words( train[\"review\"])\n",
    "# print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train1[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(num_reviews):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )\n",
    "\n",
    "clean_test_reviews = [] \n",
    "\n",
    "for i in range(num_reviews):\n",
    "#     if( (i+1) % 1000 == 0 ):\n",
    "#         print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quite enjoyed movie two reasons first gives insight world loyalism northern ireland rarely treated movies tell us republican struggle second reason performances actors thought gave honest convincing portrayals seedy underworld many people hear outside native shores entertaining ganster movie stellar performances northern irish actors cast wont move earth although may slightly open peoples eyes murky world loyalist paramilitaries'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "#         strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "#         ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "#         stop_words = 'english')\n",
    "\n",
    "# train_data_features = tfv.fit_transform(clean_train_reviews).toarray()\n",
    "\n",
    "# test_data_features = tfv.fit_transform(clean_test_reviews).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "all = clean_train_reviews + clean_test_reviews\n",
    "all_feature = tfv.fit_transform(all).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "model.fit(all_feature[:num_reviews], train[\"sentiment\"])\n",
    "result = model.predict(all_feature[num_reviews:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model I train in pickle \n",
    "# Save the final model in trained_model.pkl\n",
    "import pickle\n",
    "\n",
    "# data2 = grid_sgdlogreg2\n",
    "output = open('trained_model.pkl', 'wb')\n",
    "\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(model, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array(test1['sentiment'])\n",
    "y_scores = result\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
